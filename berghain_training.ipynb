{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "berghain-header"
   },
   "source": [
    "# Berghain LSTM Policy Network Training\n",
    "\n",
    "This notebook trains an LSTM neural network to learn optimal admission decisions for the Berghain nightclub challenge using supervised learning on historical game data.\n",
    "\n",
    "## Features\n",
    "- **Supervised Learning**: Trains on successful game strategies\n",
    "- **Real-time Visualization**: Shows training progress with loss and accuracy plots\n",
    "- **Model Checkpointing**: Saves best model automatically\n",
    "- **GPU Acceleration**: Uses CUDA when available\n",
    "- **Comprehensive Logging**: Detailed training metrics\n",
    "\n",
    "## Setup Instructions\n",
    "1. Upload your game logs to the `game_logs/` directory\n",
    "2. Run all cells in order\n",
    "3. Download the trained model from the `models/` directory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "setup-section"
   },
   "source": [
    "## 1. Setup and Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "install-dependencies"
   },
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install torch torchvision numpy matplotlib scikit-learn pyyaml requests\n",
    "\n",
    "# Import standard libraries\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "import logging\n",
    "from typing import Dict, List, Tuple, Any\n",
    "from dataclasses import dataclass\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA device: {torch.cuda.get_device_name()}\")\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Training on: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "upload-section"
   },
   "source": [
    "## 2. Upload and Prepare Data\n",
    "\n",
    "Upload your game logs to Google Colab. The logs should be in JSON format from successful games."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "upload-data"
   },
   "outputs": [],
   "source": [
    "# Create directories\n",
    "!mkdir -p game_logs models data\n",
    "\n",
    "# For Google Colab, upload files\n",
    "from google.colab import files\n",
    "import zipfile\n",
    "import shutil\n",
    "\n",
    "print(\"Please upload your game logs.\")\n",
    "print(\"You can either:\")\n",
    "print(\"1. Upload a ZIP file containing all game logs\")\n",
    "print(\"2. Upload individual JSON files\")\n",
    "\n",
    "uploaded = files.upload()\n",
    "\n",
    "# Process uploaded files\n",
    "for filename in uploaded.keys():\n",
    "    if filename.endswith('.zip'):\n",
    "        # Extract ZIP file\n",
    "        with zipfile.ZipFile(filename, 'r') as zip_ref:\n",
    "            zip_ref.extractall('game_logs/')\n",
    "        print(f\"Extracted {filename} to game_logs/\")\n",
    "    elif filename.endswith('.json'):\n",
    "        # Move JSON file to game_logs\n",
    "        shutil.move(filename, f'game_logs/{filename}')\n",
    "        print(f\"Moved {filename} to game_logs/\")\n",
    "\n",
    "# List uploaded files\n",
    "game_files = [f for f in os.listdir('game_logs') if f.endswith('.json') and not 'consolidated' in f]\n",
    "print(f\"\\nFound {len(game_files)} game log files\")\n",
    "if game_files:\n",
    "    print(\"Sample files:\")\n",
    "    for f in game_files[:5]:\n",
    "        print(f\"  {f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "model-section"
   },
   "source": [
    "## 3. Define Model Architecture\n",
    "\n",
    "LSTM-based policy network for sequential decision making."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "define-model"
   },
   "outputs": [],
   "source": [
    "class LSTMPolicyNetwork(nn.Module):\n",
    "    \"\"\"\n",
    "    LSTM-based policy network for sequential decision making in Berghain game.\n",
    "    \n",
    "    Architecture:\n",
    "    - LSTM layers for temporal modeling\n",
    "    - Policy head for action probabilities (accept/reject)\n",
    "    - Value head for state value estimation (for PPO)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        input_dim: int = 8,\n",
    "        hidden_dim: int = 128,\n",
    "        lstm_layers: int = 2,\n",
    "        dropout: float = 0.1\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.lstm_layers = lstm_layers\n",
    "        \n",
    "        # LSTM backbone for sequential modeling\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=lstm_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout if lstm_layers > 1 else 0\n",
    "        )\n",
    "        \n",
    "        # Policy head (action probabilities)\n",
    "        self.policy_head = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 2),  # Binary: [reject_prob, accept_prob]\n",
    "            nn.Softmax(dim=-1)\n",
    "        )\n",
    "        \n",
    "        # Value head (state value estimation)\n",
    "        self.value_head = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 1)  # Single scalar value\n",
    "        )\n",
    "        \n",
    "        # Initialize weights\n",
    "        self._init_weights()\n",
    "    \n",
    "    def _init_weights(self):\n",
    "        \"\"\"Initialize network weights using Xavier initialization.\"\"\"\n",
    "        for name, param in self.named_parameters():\n",
    "            if 'weight' in name:\n",
    "                nn.init.xavier_uniform_(param)\n",
    "            elif 'bias' in name:\n",
    "                nn.init.zeros_(param)\n",
    "    \n",
    "    def forward(\n",
    "        self, \n",
    "        x: torch.Tensor, \n",
    "        hidden: tuple = None\n",
    "    ) -> Tuple[torch.Tensor, torch.Tensor, tuple]:\n",
    "        # LSTM forward pass\n",
    "        lstm_out, hidden_new = self.lstm(x, hidden)\n",
    "        \n",
    "        # Apply heads\n",
    "        policy = self.policy_head(lstm_out)\n",
    "        value = self.value_head(lstm_out)\n",
    "        \n",
    "        return policy, value, hidden_new\n",
    "    \n",
    "    def set_training_history(self, history_dict: dict) -> None:\n",
    "        \"\"\"Set training history for the model.\"\"\"\n",
    "        self.training_history = history_dict\n",
    "    \n",
    "    def get_training_history(self) -> dict:\n",
    "        \"\"\"Get training history from the model.\"\"\"\n",
    "        return getattr(self, 'training_history', {})\n",
    "\n",
    "print(\"Model architecture defined successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "preprocessing-section"
   },
   "source": [
    "## 4. Data Preprocessing\n",
    "\n",
    "Convert game logs into training sequences for the LSTM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "preprocessing"
   },
   "outputs": [],
   "source": [
    "class GameDataPreprocessor:\n",
    "    \"\"\"\n",
    "    Preprocesses game log data for supervised learning.\n",
    "    \n",
    "    Converts decision sequences from game logs into training data where:\n",
    "    - X: Sequential game state features\n",
    "    - y: Expert decision labels (0=reject, 1=accept)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, sequence_length: int = 50):\n",
    "        self.sequence_length = sequence_length\n",
    "        self.feature_dim = 8  # Matches StateEncoder feature count\n",
    "        \n",
    "    def load_game_logs(self, log_directory: str) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Load all individual game JSON files from directory.\"\"\"\n",
    "        games = []\n",
    "        \n",
    "        for filename in os.listdir(log_directory):\n",
    "            if (filename.startswith('game_') and \n",
    "                filename.endswith('.json') and \n",
    "                'consolidated' not in filename):\n",
    "                \n",
    "                filepath = os.path.join(log_directory, filename)\n",
    "                try:\n",
    "                    with open(filepath, 'r') as f:\n",
    "                        game_data = json.load(f)\n",
    "                        # Only include successful games for supervised learning\n",
    "                        if game_data.get('success', False):\n",
    "                            games.append(game_data)\n",
    "                except Exception as e:\n",
    "                    logger.warning(f\"Error loading {filename}: {e}\")\n",
    "        \n",
    "        logger.info(f\"Loaded {len(games)} successful games from {log_directory}\")\n",
    "        return games\n",
    "    \n",
    "    def extract_features_and_labels(self, game_data: Dict[str, Any]) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        \"\"\"Extract feature sequences and decision labels from a single game.\"\"\"\n",
    "        decisions = game_data['decisions']\n",
    "        features = []\n",
    "        labels = []\n",
    "        \n",
    "        # Track game state progression\n",
    "        admitted_count = 0\n",
    "        rejected_count = 0\n",
    "        constraints = game_data['constraints']\n",
    "        \n",
    "        # Get constraint targets\n",
    "        young_target = next((c['min_count'] for c in constraints if c['attribute'] == 'young'), 600)\n",
    "        well_dressed_target = next((c['min_count'] for c in constraints if c['attribute'] == 'well_dressed'), 600)\n",
    "        \n",
    "        # Track admitted attributes\n",
    "        young_admitted = 0\n",
    "        well_dressed_admitted = 0\n",
    "        \n",
    "        for i, decision in enumerate(decisions):\n",
    "            person_attrs = decision['attributes']\n",
    "            decision_made = decision['decision']\n",
    "            \n",
    "            # Person attributes\n",
    "            well_dressed = 1.0 if person_attrs.get('well_dressed', False) else 0.0\n",
    "            young = 1.0 if person_attrs.get('young', False) else 0.0\n",
    "            \n",
    "            # Constraint progress\n",
    "            constraint_progress_y = min(young_admitted / young_target, 1.0) if young_target > 0 else 1.0\n",
    "            constraint_progress_w = min(well_dressed_admitted / well_dressed_target, 1.0) if well_dressed_target > 0 else 1.0\n",
    "            \n",
    "            # Capacity and rejection ratios\n",
    "            total_decisions = admitted_count + rejected_count\n",
    "            capacity_ratio = admitted_count / 1000.0  # Max capacity\n",
    "            rejection_ratio = rejected_count / 20000.0 if total_decisions > 0 else 0.0  # Max rejections\n",
    "            \n",
    "            # Game phase\n",
    "            if admitted_count < 300:\n",
    "                game_phase = 0.0  # Early\n",
    "            elif admitted_count < 700:\n",
    "                game_phase = 0.5  # Mid\n",
    "            else:\n",
    "                game_phase = 1.0  # Late\n",
    "            \n",
    "            # Person index normalized\n",
    "            person_index_norm = min(i / 25000, 1.0)\n",
    "            \n",
    "            # Create feature vector\n",
    "            feature_vector = np.array([\n",
    "                well_dressed, young, constraint_progress_y, constraint_progress_w,\n",
    "                capacity_ratio, rejection_ratio, game_phase, person_index_norm\n",
    "            ], dtype=np.float32)\n",
    "            \n",
    "            features.append(feature_vector)\n",
    "            labels.append(1 if decision_made else 0)\n",
    "            \n",
    "            # Update game state for next iteration\n",
    "            if decision_made:\n",
    "                admitted_count += 1\n",
    "                if person_attrs.get('young', False):\n",
    "                    young_admitted += 1\n",
    "                if person_attrs.get('well_dressed', False):\n",
    "                    well_dressed_admitted += 1\n",
    "            else:\n",
    "                rejected_count += 1\n",
    "        \n",
    "        return np.array(features), np.array(labels, dtype=np.int64)\n",
    "    \n",
    "    def create_sequences(self, features: np.ndarray, labels: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        \"\"\"Create fixed-length sequences for LSTM training.\"\"\"\n",
    "        if len(features) <= self.sequence_length:\n",
    "            # Pad short sequences\n",
    "            pad_length = self.sequence_length - len(features)\n",
    "            padded_features = np.pad(features, ((0, pad_length), (0, 0)), mode='constant')\n",
    "            padded_labels = np.pad(labels, (0, pad_length), mode='constant')\n",
    "            return padded_features[np.newaxis, :, :], padded_labels[np.newaxis, :]\n",
    "        \n",
    "        # Create overlapping windows for longer sequences\n",
    "        sequences_features = []\n",
    "        sequences_labels = []\n",
    "        \n",
    "        for i in range(0, len(features) - self.sequence_length + 1, self.sequence_length // 2):\n",
    "            end_idx = i + self.sequence_length\n",
    "            sequences_features.append(features[i:end_idx])\n",
    "            sequences_labels.append(labels[i:end_idx])\n",
    "        \n",
    "        return np.array(sequences_features), np.array(sequences_labels)\n",
    "    \n",
    "    def prepare_dataset(self, games: List[Dict[str, Any]]) -> Tuple[List[torch.Tensor], List[torch.Tensor]]:\n",
    "        \"\"\"Prepare complete dataset from list of games.\"\"\"\n",
    "        all_sequences = []\n",
    "        all_labels = []\n",
    "        \n",
    "        for game in games:\n",
    "            try:\n",
    "                features, labels = self.extract_features_and_labels(game)\n",
    "                seq_features, seq_labels = self.create_sequences(features, labels)\n",
    "                \n",
    "                # Convert to tensors and add to lists\n",
    "                for i in range(len(seq_features)):\n",
    "                    all_sequences.append(torch.tensor(seq_features[i], dtype=torch.float32))\n",
    "                    all_labels.append(torch.tensor(seq_labels[i], dtype=torch.long))\n",
    "                    \n",
    "            except Exception as e:\n",
    "                logger.warning(f\"Error processing game {game.get('game_id', 'unknown')}: {e}\")\n",
    "        \n",
    "        logger.info(f\"Created {len(all_sequences)} training sequences\")\n",
    "        return all_sequences, all_labels\n",
    "\n",
    "\n",
    "class SequenceDataset(Dataset):\n",
    "    \"\"\"Dataset class for sequence training data.\"\"\"\n",
    "    \n",
    "    def __init__(self, data_tuples: List[Tuple[torch.Tensor, torch.Tensor]]):\n",
    "        self.sequences = [item[0] for item in data_tuples]\n",
    "        self.labels = [item[1] for item in data_tuples]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.sequences[idx], self.labels[idx]\n",
    "\n",
    "\n",
    "# Prepare the training data\n",
    "print(\"Loading and preprocessing game data...\")\n",
    "processor = GameDataPreprocessor(sequence_length=50)\n",
    "\n",
    "# Load games\n",
    "games = processor.load_game_logs('game_logs')\n",
    "if not games:\n",
    "    raise ValueError(\"No successful games found in game_logs directory\")\n",
    "\n",
    "# Prepare dataset\n",
    "sequences, labels = processor.prepare_dataset(games)\n",
    "\n",
    "# Split into train/validation\n",
    "train_sequences, val_sequences, train_labels, val_labels = train_test_split(\n",
    "    sequences, labels, test_size=0.2, random_state=42, shuffle=True\n",
    ")\n",
    "\n",
    "# Combine into tuples\n",
    "train_data = list(zip(train_sequences, train_labels))\n",
    "val_data = list(zip(val_sequences, val_labels))\n",
    "\n",
    "print(f\"Training samples: {len(train_data)}\")\n",
    "print(f\"Validation samples: {len(val_data)}\")\n",
    "\n",
    "# Create data loaders\n",
    "batch_size = 32\n",
    "train_dataset = SequenceDataset(train_data)\n",
    "val_dataset = SequenceDataset(val_data)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, drop_last=False)\n",
    "\n",
    "print(f\"Training batches: {len(train_loader)}\")\n",
    "print(f\"Validation batches: {len(val_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "training-section"
   },
   "source": [
    "## 5. Training Configuration and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "training-config"
   },
   "outputs": [],
   "source": [
    "# Training configuration\n",
    "@dataclass\n",
    "class TrainingConfig:\n",
    "    # Model parameters\n",
    "    input_dim: int = 8\n",
    "    hidden_dim: int = 128\n",
    "    lstm_layers: int = 2\n",
    "    dropout: float = 0.1\n",
    "    \n",
    "    # Training parameters\n",
    "    epochs: int = 50\n",
    "    learning_rate: float = 0.001\n",
    "    weight_decay: float = 1e-5\n",
    "    patience: int = 10  # Early stopping patience\n",
    "    \n",
    "    # Paths\n",
    "    save_path: str = 'models/lstm_berghain'\n",
    "\n",
    "config = TrainingConfig()\n",
    "\n",
    "# Initialize model\n",
    "model = LSTMPolicyNetwork(\n",
    "    input_dim=config.input_dim,\n",
    "    hidden_dim=config.hidden_dim,\n",
    "    lstm_layers=config.lstm_layers,\n",
    "    dropout=config.dropout\n",
    ").to(device)\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Model parameters: {total_params:,} (trainable: {trainable_params:,})\")\n",
    "\n",
    "# Initialize optimizer and criterion\n",
    "optimizer = optim.Adam(\n",
    "    model.parameters(), \n",
    "    lr=config.learning_rate,\n",
    "    weight_decay=config.weight_decay\n",
    ")\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Training history tracking\n",
    "training_history = {\n",
    "    'train_loss': [],\n",
    "    'val_loss': [],\n",
    "    'train_accuracy': [],\n",
    "    'val_accuracy': [],\n",
    "    'epochs': [],\n",
    "    'best_epoch': 0,\n",
    "    'best_val_accuracy': 0.0\n",
    "}\n",
    "\n",
    "print(\"Training setup completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "training-loop-section"
   },
   "source": [
    "## 6. Training Loop with Real-time Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "training-loop",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Training and validation functions\n",
    "def train_epoch(model, train_loader, optimizer, criterion, device):\n",
    "    \"\"\"Train for one epoch.\"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    correct_predictions = 0\n",
    "    total_predictions = 0\n",
    "    \n",
    "    for batch_idx, (sequences, labels) in enumerate(train_loader):\n",
    "        sequences = sequences.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        # Zero gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        policy, _, _ = model(sequences)\n",
    "        \n",
    "        # Reshape for loss calculation\n",
    "        policy_flat = policy.view(-1, 2)  # (batch * seq_len, 2)\n",
    "        labels_flat = labels.view(-1)  # (batch * seq_len,)\n",
    "        \n",
    "        # Calculate loss\n",
    "        loss = criterion(policy_flat, labels_flat)\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Track metrics\n",
    "        total_loss += loss.item()\n",
    "        _, predicted = torch.max(policy_flat, 1)\n",
    "        total_predictions += labels_flat.size(0)\n",
    "        correct_predictions += (predicted == labels_flat).sum().item()\n",
    "    \n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    accuracy = 100.0 * correct_predictions / total_predictions\n",
    "    \n",
    "    return avg_loss, accuracy\n",
    "\n",
    "\n",
    "def validate_epoch(model, val_loader, criterion, device):\n",
    "    \"\"\"Validate for one epoch.\"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    correct_predictions = 0\n",
    "    total_predictions = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for sequences, labels in val_loader:\n",
    "            sequences = sequences.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            policy, _, _ = model(sequences)\n",
    "            \n",
    "            # Reshape for loss calculation\n",
    "            policy_flat = policy.view(-1, 2)\n",
    "            labels_flat = labels.view(-1)\n",
    "            \n",
    "            # Calculate loss\n",
    "            loss = criterion(policy_flat, labels_flat)\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            # Track accuracy\n",
    "            _, predicted = torch.max(policy_flat, 1)\n",
    "            total_predictions += labels_flat.size(0)\n",
    "            correct_predictions += (predicted == labels_flat).sum().item()\n",
    "    \n",
    "    avg_loss = total_loss / len(val_loader)\n",
    "    accuracy = 100.0 * correct_predictions / total_predictions\n",
    "    \n",
    "    return avg_loss, accuracy\n",
    "\n",
    "\n",
    "def plot_training_progress(history):\n",
    "    \"\"\"Plot training progress.\"\"\"\n",
    "    if not history['epochs']:\n",
    "        return\n",
    "    \n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "    \n",
    "    epochs = history['epochs']\n",
    "    \n",
    "    # Plot losses\n",
    "    ax1.plot(epochs, history['train_loss'], label='Train Loss', color='blue')\n",
    "    ax1.plot(epochs, history['val_loss'], label='Validation Loss', color='red')\n",
    "    ax1.set_title('Training and Validation Loss')\n",
    "    ax1.set_xlabel('Epoch')\n",
    "    ax1.set_ylabel('Loss')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot accuracies\n",
    "    ax2.plot(epochs, history['train_accuracy'], label='Train Accuracy', color='green')\n",
    "    ax2.plot(epochs, history['val_accuracy'], label='Validation Accuracy', color='orange')\n",
    "    ax2.set_title('Training and Validation Accuracy')\n",
    "    ax2.set_xlabel('Epoch')\n",
    "    ax2.set_ylabel('Accuracy (%)')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Main training loop\n",
    "print(\"Starting LSTM training pipeline...\")\n",
    "print(\"Preparing training data...\")\n",
    "\n",
    "best_val_accuracy = 0.0\n",
    "patience_counter = 0\n",
    "start_time = datetime.now()\n",
    "\n",
    "for epoch in range(1, config.epochs + 1):\n",
    "    epoch_start = datetime.now()\n",
    "    \n",
    "    # Train and validate\n",
    "    train_loss, train_acc = train_epoch(model, train_loader, optimizer, criterion, device)\n",
    "    val_loss, val_acc = validate_epoch(model, val_loader, criterion, device)\n",
    "    \n",
    "    # Update history\n",
    "    training_history['train_loss'].append(train_loss)\n",
    "    training_history['val_loss'].append(val_loss)\n",
    "    training_history['train_accuracy'].append(train_acc)\n",
    "    training_history['val_accuracy'].append(val_acc)\n",
    "    training_history['epochs'].append(epoch)\n",
    "    \n",
    "    # Check for best model\n",
    "    is_best = val_acc > best_val_accuracy\n",
    "    if is_best:\n",
    "        best_val_accuracy = val_acc\n",
    "        training_history['best_epoch'] = epoch\n",
    "        training_history['best_val_accuracy'] = val_acc\n",
    "        patience_counter = 0\n",
    "        \n",
    "        # Save best model\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'training_history': training_history,\n",
    "            'config': config.__dict__\n",
    "        }, f\"{config.save_path}_best.pth\")\n",
    "        \n",
    "        print(f\"‚úÖ New best model saved with validation accuracy: {val_acc:.2f}%\")\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "    \n",
    "    # Log progress\n",
    "    epoch_time = (datetime.now() - epoch_start).total_seconds()\n",
    "    print(f\"Epoch {epoch:2d}/{config.epochs} | \"\n",
    "          f\"Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.1f}% | \"\n",
    "          f\"Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.1f}% | \"\n",
    "          f\"Time: {epoch_time:.1f}s\")\n",
    "    \n",
    "    # Plot progress every 10 epochs\n",
    "    if epoch % 10 == 0:\n",
    "        plot_training_progress(training_history)\n",
    "    \n",
    "    # Early stopping check\n",
    "    if patience_counter >= config.patience:\n",
    "        print(f\"üõë Early stopping triggered after {epoch} epochs\")\n",
    "        break\n",
    "\n",
    "# Training completed\n",
    "total_time = (datetime.now() - start_time).total_seconds()\n",
    "print(f\"\\nüéâ Training completed in {total_time/60:.1f} minutes\")\n",
    "print(f\"üìä Best validation accuracy: {best_val_accuracy:.2f}% at epoch {training_history['best_epoch']}\")\n",
    "\n",
    "# Set training history in model\n",
    "model.set_training_history(training_history)\n",
    "\n",
    "# Save final model\n",
    "torch.save({\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'training_history': training_history,\n",
    "    'config': config.__dict__\n",
    "}, f\"{config.save_path}_final.pth\")\n",
    "\n",
    "print(f\"üíæ Model saved to {config.save_path}_final.pth\")\n",
    "print(\"üìà Training history saved in model\")\n",
    "\n",
    "# Final training plot\n",
    "plot_training_progress(training_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "evaluation-section"
   },
   "source": [
    "## 7. Model Evaluation and Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "model-evaluation"
   },
   "outputs": [],
   "source": [
    "# Load the best model\n",
    "checkpoint = torch.load(f\"{config.save_path}_best.pth\", map_location=device)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "model.eval()\n",
    "\n",
    "print(\"Testing model inference...\")\n",
    "\n",
    "# Test model on validation set\n",
    "test_correct = 0\n",
    "test_total = 0\n",
    "accept_correct = 0\n",
    "accept_total = 0\n",
    "reject_correct = 0\n",
    "reject_total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for sequences, labels in val_loader:\n",
    "        sequences = sequences.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        policy, _, _ = model(sequences)\n",
    "        policy_flat = policy.view(-1, 2)\n",
    "        labels_flat = labels.view(-1)\n",
    "        \n",
    "        _, predicted = torch.max(policy_flat, 1)\n",
    "        \n",
    "        # Overall accuracy\n",
    "        test_total += labels_flat.size(0)\n",
    "        test_correct += (predicted == labels_flat).sum().item()\n",
    "        \n",
    "        # Per-class accuracy\n",
    "        accept_mask = labels_flat == 1\n",
    "        reject_mask = labels_flat == 0\n",
    "        \n",
    "        if accept_mask.sum() > 0:\n",
    "            accept_total += accept_mask.sum().item()\n",
    "            accept_correct += (predicted[accept_mask] == labels_flat[accept_mask]).sum().item()\n",
    "        \n",
    "        if reject_mask.sum() > 0:\n",
    "            reject_total += reject_mask.sum().item()\n",
    "            reject_correct += (predicted[reject_mask] == labels_flat[reject_mask]).sum().item()\n",
    "\n",
    "# Calculate metrics\n",
    "overall_accuracy = 100.0 * test_correct / test_total\n",
    "accept_accuracy = 100.0 * accept_correct / accept_total if accept_total > 0 else 0\n",
    "reject_accuracy = 100.0 * reject_correct / reject_total if reject_total > 0 else 0\n",
    "\n",
    "print(\"\\nüìä Final Model Performance:\")\n",
    "print(f\"  Overall Accuracy: {overall_accuracy:.2f}%\")\n",
    "print(f\"  Accept Accuracy: {accept_accuracy:.2f}% ({accept_correct}/{accept_total})\")\n",
    "print(f\"  Reject Accuracy: {reject_accuracy:.2f}% ({reject_correct}/{reject_total})\")\n",
    "print(f\"  Total Test Samples: {test_total:,}\")\n",
    "\n",
    "# Display training summary\n",
    "print(\"\\nüìà Training Summary:\")\n",
    "print(f\"  Best Epoch: {training_history['best_epoch']}\")\n",
    "print(f\"  Best Validation Accuracy: {training_history['best_val_accuracy']:.2f}%\")\n",
    "print(f\"  Final Training Loss: {training_history['train_loss'][-1]:.4f}\")\n",
    "print(f\"  Final Validation Loss: {training_history['val_loss'][-1]:.4f}\")\n",
    "print(f\"  Total Epochs Trained: {len(training_history['epochs'])}\")\n",
    "\n",
    "print(\"\\nTraining complete! Model is ready for inference. üéâ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "visualization-section"
   },
   "source": [
    "## 8. Final Training Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "final-plots"
   },
   "outputs": [],
   "source": [
    "# Plot comprehensive training history\n",
    "def plot_comprehensive_history(history):\n",
    "    \"\"\"Plot comprehensive training history.\"\"\"\n",
    "    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    \n",
    "    epochs = history['epochs']\n",
    "    \n",
    "    # Plot losses\n",
    "    ax1.plot(epochs, history['train_loss'], label='Train Loss', color='blue', linewidth=2)\n",
    "    ax1.plot(epochs, history['val_loss'], label='Validation Loss', color='red', linewidth=2)\n",
    "    ax1.axvline(x=history['best_epoch'], color='green', linestyle='--', alpha=0.7, label='Best Epoch')\n",
    "    ax1.set_title('Training and Validation Loss', fontsize=14, fontweight='bold')\n",
    "    ax1.set_xlabel('Epoch')\n",
    "    ax1.set_ylabel('Loss')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot accuracies\n",
    "    ax2.plot(epochs, history['train_accuracy'], label='Train Accuracy', color='green', linewidth=2)\n",
    "    ax2.plot(epochs, history['val_accuracy'], label='Validation Accuracy', color='orange', linewidth=2)\n",
    "    ax2.axvline(x=history['best_epoch'], color='green', linestyle='--', alpha=0.7, label='Best Epoch')\n",
    "    ax2.set_title('Training and Validation Accuracy', fontsize=14, fontweight='bold')\n",
    "    ax2.set_xlabel('Epoch')\n",
    "    ax2.set_ylabel('Accuracy (%)')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot overfitting indicator\n",
    "    loss_diff = [v - t for t, v in zip(history['train_loss'], history['val_loss'])]\n",
    "    ax3.plot(epochs, loss_diff, label='Val - Train Loss', color='purple', linewidth=2)\n",
    "    ax3.axhline(y=0, color='black', linestyle='-', alpha=0.5)\n",
    "    ax3.axvline(x=history['best_epoch'], color='green', linestyle='--', alpha=0.7)\n",
    "    ax3.set_title('Overfitting Indicator (Val Loss - Train Loss)', fontsize=14, fontweight='bold')\n",
    "    ax3.set_xlabel('Epoch')\n",
    "    ax3.set_ylabel('Loss Difference')\n",
    "    ax3.legend()\n",
    "    ax3.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Summary statistics\n",
    "    ax4.text(0.05, 0.90, f\"Best Epoch: {history['best_epoch']}\", transform=ax4.transAxes, fontsize=12, fontweight='bold')\n",
    "    ax4.text(0.05, 0.80, f\"Best Val Accuracy: {history['best_val_accuracy']:.2f}%\", transform=ax4.transAxes, fontsize=12)\n",
    "    ax4.text(0.05, 0.70, f\"Final Train Loss: {history['train_loss'][-1]:.4f}\", transform=ax4.transAxes, fontsize=12)\n",
    "    ax4.text(0.05, 0.60, f\"Final Val Loss: {history['val_loss'][-1]:.4f}\", transform=ax4.transAxes, fontsize=12)\n",
    "    ax4.text(0.05, 0.50, f\"Total Epochs: {len(history['epochs'])}\", transform=ax4.transAxes, fontsize=12)\n",
    "    ax4.text(0.05, 0.40, f\"Training Samples: {len(train_data):,}\", transform=ax4.transAxes, fontsize=12)\n",
    "    ax4.text(0.05, 0.30, f\"Validation Samples: {len(val_data):,}\", transform=ax4.transAxes, fontsize=12)\n",
    "    ax4.text(0.05, 0.20, f\"Model Parameters: {total_params:,}\", transform=ax4.transAxes, fontsize=12)\n",
    "    ax4.text(0.05, 0.10, f\"Device: {device}\", transform=ax4.transAxes, fontsize=12)\n",
    "    ax4.set_title('Training Summary', fontsize=14, fontweight='bold')\n",
    "    ax4.axis('off')\n",
    "    \n",
    "    plt.suptitle('Berghain LSTM Training Results', fontsize=16, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Generate final comprehensive plot\n",
    "plot_comprehensive_history(training_history)\n",
    "\n",
    "# Save training history as JSON for later analysis\n",
    "with open(f\"{config.save_path}_history.json\", 'w') as f:\n",
    "    json.dump(training_history, f, indent=2)\n",
    "\n",
    "print(f\"üìÅ Training history saved to {config.save_path}_history.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "download-section"
   },
   "source": [
    "## 9. Download Trained Model\n",
    "\n",
    "Download the trained model files to use in your local environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "download-model"
   },
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "import zipfile\n",
    "import os\n",
    "\n",
    "# Create a ZIP file with all model artifacts\n",
    "zip_filename = 'berghain_lstm_trained_model.zip'\n",
    "\n",
    "with zipfile.ZipFile(zip_filename, 'w') as zipf:\n",
    "    # Add model files\n",
    "    if os.path.exists(f\"{config.save_path}_best.pth\"):\n",
    "        zipf.write(f\"{config.save_path}_best.pth\", \"lstm_berghain_best.pth\")\n",
    "    if os.path.exists(f\"{config.save_path}_final.pth\"):\n",
    "        zipf.write(f\"{config.save_path}_final.pth\", \"lstm_berghain_final.pth\")\n",
    "    if os.path.exists(f\"{config.save_path}_history.json\"):\n",
    "        zipf.write(f\"{config.save_path}_history.json\", \"training_history.json\")\n",
    "\n",
    "print(f\"üì¶ Model files packaged in {zip_filename}\")\n",
    "print(\"\\nüéØ Download includes:\")\n",
    "print(\"  ‚Ä¢ lstm_berghain_best.pth - Best performing model checkpoint\")\n",
    "print(\"  ‚Ä¢ lstm_berghain_final.pth - Final training checkpoint\")\n",
    "print(\"  ‚Ä¢ training_history.json - Complete training metrics\")\n",
    "\n",
    "# Download the ZIP file\n",
    "files.download(zip_filename)\n",
    "\n",
    "print(\"\\n‚úÖ Download started! Check your browser's download folder.\")\n",
    "print(\"\\nüöÄ Model is ready to use for Berghain game inference!\")\n",
    "print(\"\\nüí° Usage in your local environment:\")\n",
    "print(\"```python\")\n",
    "print(\"import torch\")\n",
    "print(\"from berghain.training.lstm_policy import LSTMPolicyNetwork\")\n",
    "print(\"\")\n",
    "print(\"# Load the trained model\")\n",
    "print(\"model = LSTMPolicyNetwork()\")\n",
    "print(\"checkpoint = torch.load('lstm_berghain_best.pth')\")\n",
    "print(\"model.load_state_dict(checkpoint['model_state_dict'])\")\n",
    "print(\"\")\n",
    "print(\"# Access training history\")\n",
    "print(\"history = checkpoint['training_history']\")\n",
    "print(\"print(f'Best accuracy: {history[\\\"best_val_accuracy\\\"]:.2f}%')\")\n",
    "print(\"```\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "berghain_training.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}